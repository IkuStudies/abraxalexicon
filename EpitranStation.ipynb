{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1061T-5OS6ZtwXk8csXYHO807ovcOgEGA",
      "authorship_tag": "ABX9TyP5txb0TkY3JLFrij3bm//s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkuStudies/abraxalexicon/blob/main/EpitranStation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "these programs all do the same thing.  input a word list, output a pronunciation list aligned on the same line numbers.  change the .txt to .csv and upload them onto mysql.  if you see an epitran supported language i don't cover that you have a word list for and want to add to the abraxa lexicon, please shoot me an email at ikustudies@gmail.com with the wordlist or if you are feeling frisky pop over here and find the code you're looking for and modify the code with the code in this first list here, mount google drive with your word list.  find it on the left here, right click the file copy path, and paste it in where the input file is and name your output file.  and send me both the word list and the pronunciation list, and save the endings as .csv\n",
        "\n",
        "and I'm working on getting the full multilingual translation station going to for these lists to translate into english for the definition data parameter.\n",
        "\n",
        "the abraxa lexicon will be searchable and i'm going to be using the dataset to train machine learning models to basically find patterns and cognates.\n",
        "\n",
        "epitran was a huge help here, i was approaching mysql all wrong, but doing it this way is fastest I've found.  \n",
        "\n",
        "other languages i'm decoding into word lists are many native american languages, aramaic, and middle egyptian, icelandic, and possibly elder and younger futhark.  i want to get the database up and running and then maybe raise a little funds so i can pay my rent and really focus on getting the quality of this linguistics database perfected pronto.  feel free to email me with insights or whatever.  i'm also on telegram at t.me/ikustudygroup doing lots of things, right now i'm doing this, but doing lots of other interesting things too, drop by and nerd out if you want.  i'm alllllmost done with this."
      ],
      "metadata": {
        "id": "0BTuHEMHuNlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Thank You Epitran](https://pypi.org/project/epitran/)"
      ],
      "metadata": {
        "id": "5wEJmNKFmvcA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Languages supported by epitran (code followed by language):\n",
        "\n",
        "aii-Syrc \tAssyrian Neo-Aramaic\n",
        "amh-Ethi \tAmharic\n",
        "amh-Ethi-pp \tAmharic (more phonetic)\n",
        "amh-Ethi-red \tAmharic (reduced)\n",
        "ara-Arab \tLiterary Arabic\n",
        "ava-Cyrl \tAvaric\n",
        "aze-Cyrl \tAzerbaijani (Cyrillic)\n",
        "aze-Latn \tAzerbaijani (Latin)\n",
        "ben-Beng \tBengali\n",
        "ben-Beng-red \tBengali (reduced)\n",
        "bxk-Latn \tBukusu\n",
        "cat-Latn \tCatalan\n",
        "ceb-Latn \tCebuano\n",
        "ces-Latn \tCzech\n",
        "cjy-Latn \tJin (Wiktionary)\n",
        "cmn-Hans \tMandarin (Simplified)*\n",
        "cmn-Hant \tMandarin (Traditional)*\n",
        "cmn-Latn \tMandarin (Pinyin)*\n",
        "ckb-Arab \tSorani\n",
        "csb-Latn \tKashubian\n",
        "deu-Latn \tGerman\n",
        "deu-Latn-np \tGerman†\n",
        "deu-Latn-nar \tGerman (more phonetic)\n",
        "eng-Latn \tEnglish‡\n",
        "fas-Arab \tFarsi (Perso-Arabic)\n",
        "fra-Latn \tFrench\n",
        "fra-Latn-np \tFrench†\n",
        "fra-Latn-p \tFrench (more phonetic)\n",
        "ful-Latn \tFulah\n",
        "gan-Latn \tGan (Wiktionary)\n",
        "got-Latn \tGothic\n",
        "hak-Latn \tHakka (pha̍k-fa-sṳ)\n",
        "hau-Latn \tHausa\n",
        "hin-Deva \tHindi\n",
        "hmn-Latn \tHmong\n",
        "hrv-Latn \tCroatian\n",
        "hsn-Latn \tXiang (Wiktionary)\n",
        "hun-Latn \tHungarian\n",
        "ilo-Latn \tIlocano\n",
        "ind-Latn \tIndonesian\n",
        "ita-Latn \tItalian\n",
        "jam-Latn \tJamaican\n",
        "jav-Latn \tJavanese\n",
        "kaz-Cyrl \tKazakh (Cyrillic)\n",
        "kaz-Cyrl-bab \tKazakh (Cyrillic—Babel)\n",
        "kaz-Latn \tKazakh (Latin)\n",
        "kbd-Cyrl \tKabardian\n",
        "khm-Khmr \tKhmer\n",
        "kin-Latn \tKinyarwanda\n",
        "kir-Arab \tKyrgyz (Perso-Arabic)\n",
        "kir-Cyrl \tKyrgyz (Cyrillic)\n",
        "kir-Latn \tKyrgyz (Latin)\n",
        "kmr-Latn \tKurmanji\n",
        "kmr-Latn-red \tKurmanji (reduced)\n",
        "lao-Laoo \tLao\n",
        "lsm-Latn \tSaamia\n",
        "ltc-Latn-bax \tMiddle Chinese (Baxter and Sagart 2014)\n",
        "mal-Mlym \tMalayalam\n",
        "mar-Deva \tMarathi\n",
        "mlt-Latn \tMaltese\n",
        "mon-Cyrl \tMongolian (Cyrillic)\n",
        "mri-Latn \tMaori\n",
        "msa-Latn \tMalay\n",
        "mya-Mymr \tBurmese\n",
        "nan-Latn \tHokkien (pe̍h-oē-jī)\n",
        "nan-Latn-tl \tHokkien (Tâi-lô)\n",
        "nld-Latn \tDutch\n",
        "nya-Latn \tChichewa\n",
        "ood-Lat-alv \tTohono O'odham\n",
        "ood-Latn-sax \tTohono O'odham\n",
        "ori-Orya \tOdia\n",
        "orm-Latn \tOromo\n",
        "pan-Guru \tPunjabi (Eastern)\n",
        "pol-Latn \tPolish\n",
        "por-Latn \tPortuguese\n",
        "ron-Latn \tRomanian\n",
        "run-Latn \tRundi\n",
        "rus-Cyrl \tRussian\n",
        "sag-Latn \tSango\n",
        "sin-Sinh \tSinhala\n",
        "sna-Latn \tShona\n",
        "som-Latn \tSomali\n",
        "spa-Latn \tSpanish\n",
        "spa-Latn-eu \tSpanish (Iberian)\n",
        "sqi-Latn \tAlbanian\n",
        "swa-Latn \tSwahili\n",
        "swa-Latn-red \tSwahili (reduced)\n",
        "swe-Latn \tSwedish\n",
        "tam-Taml \tTamil\n",
        "tam-Taml-red \tTamil (reduced)\n",
        "tel-Telu \tTelugu\n",
        "tgk-Cyrl \tTajik\n",
        "tgl-Latn \tTagalog\n",
        "tgl-Latn-red \tTagalog (reduced)\n",
        "tha-Thai \tThai\n",
        "tir-Ethi \tTigrinya\n",
        "tir-Ethi-pp \tTigrinya (more phonemic)\n",
        "tir-Ethi-red \tTigrinya (reduced)\n",
        "tpi-Latn \tTok Pisin\n",
        "tuk-Cyrl \tTurkmen (Cyrillic)\n",
        "tuk-Latn \tTurkmen (Latin)\n",
        "tur-Latn \tTurkish (Latin)\n",
        "tur-Latn-bab \tTurkish (Latin—Babel)\n",
        "tur-Latn-red \tTurkish (reduced)\n",
        "ukr-Cyrl \tUkranian\n",
        "urd-Arab \tUrdu\n",
        "uig-Arab \tUyghur (Perso-Arabic)\n",
        "uzb-Cyrl \tUzbek (Cyrillic)\n",
        "uzb-Latn \tUzbek (Latin)\n",
        "vie-Latn \tVietnamese\n",
        "wuu-Latn \tShanghainese Wu (Wiktionary)\n",
        "xho-Latn \tXhosa\n",
        "yor-Latn \tYoruba\n",
        "zha-Latn \tZhuang\n",
        "zul-Latn \tZulu"
      ],
      "metadata": {
        "id": "9-E7eHFoljLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnfw8afeByG2",
        "outputId": "d29fecb8-e4ee-462d-ffd2-a7f76d6368da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting epitran\n",
            "  Downloading epitran-1.24-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.9/164.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from epitran) (67.7.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from epitran) (2022.10.31)\n",
            "Collecting panphon>=0.20 (from epitran)\n",
            "  Downloading panphon-0.20.0-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marisa-trie (from epitran)\n",
            "  Downloading marisa_trie-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from epitran) (2.27.1)\n",
            "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
            "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (6.0)\n",
            "Requirement already satisfied: numpy>=1.20.2 in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (1.22.4)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from panphon>=0.20->epitran) (0.6.2)\n",
            "Collecting munkres (from panphon>=0.20->epitran)\n",
            "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->epitran) (3.4)\n",
            "Building wheels for collected packages: unicodecsv\n",
            "  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10745 sha256=02a57b1db4732f365337d09a6d021b263b2cedb9483097bceb2c501a37d57817\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/ea/66/8e45247b09052a933eb1a680b7c64802298faba58aac9b346b\n",
            "Successfully built unicodecsv\n",
            "Installing collected packages: unicodecsv, munkres, panphon, marisa-trie, epitran\n",
            "Successfully installed epitran-1.24 marisa-trie-0.8.0 munkres-1.1.4 panphon-0.20.0 unicodecsv-0.14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install epitran\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Arabic-IPA transliteration tool\n",
        "epi = epitran.Epitran('ara-Arab')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/datadump/Pictures/arabic.txt'\n",
        "output_file = 'output.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f50xegE9adcq",
        "outputId": "f164172f-de03-4643-e938-bb232a07ac01"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9196215/9196215 [07:10<00:00, 21380.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the German-IPA transliteration tool\n",
        "epi = epitran.Epitran('deu-Latn')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/datadump/Pictures/german.txt'\n",
        "output_file = 'germanpronunciation.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTT84w6zc9dz",
        "outputId": "e70f45aa-0d36-4d93-cd88-229c37b255ad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1908815/1908815 [04:07<00:00, 7708.14it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Farsi-IPA transliteration tool\n",
        "epi = epitran.Epitran('fas-Arab')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/wordlistpronunciationdataset/persian.csv'\n",
        "output_file = 'farsipronunciation.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX9ZTspJebKl",
        "outputId": "aac8833a-0aeb-4969-d06c-99652ef5c6f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 264482/264482 [00:14<00:00, 18059.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the French-IPA transliteration tool\n",
        "epi = epitran.Epitran('fra-Latn')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/wordlistpronunciationdataset/french.csv'\n",
        "output_file = 'FrenchPronunciation.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSlHSOjrg3JE",
        "outputId": "1e20f6be-84e6-4232-c443-770e9a751e56"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 404837/404837 [01:04<00:00, 6305.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Hindi-IPA transliteration tool\n",
        "epi = epitran.Epitran('hin-Deva')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/datadump/Pictures/hindi2.txt'\n",
        "output_file = 'hindipronunciations2.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "ZqM2OWcVg-vv",
        "outputId": "b36e43f5-9335-43ba-a984-684f1e2598ae"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-16bfa5a42d18>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Remove any newline characters and split the line into individual words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe0 in position 2119: invalid continuation byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Burmese-IPA transliteration tool\n",
        "epi = epitran.Epitran('mya-Mymr')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/datadump/Pictures/burmese.txt'\n",
        "output_file = 'burmesepronunciations.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAys9uJnjAIp",
        "outputId": "e4b9ca6d-359a-47de-9629-58814c47b312"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41343/41343 [00:03<00:00, 11315.31it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Dutch-IPA transliteration tool\n",
        "epi = epitran.Epitran('nld-Latn')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/datadump/Pictures/dutch.txt'\n",
        "output_file = 'dutchpronunciations.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ak2hWCZkjCPr",
        "outputId": "9cca9443-b96e-4fc4-a712-1736695c309f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 413937/413937 [00:38<00:00, 10872.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Turkish-IPA transliteration tool\n",
        "epi = epitran.Epitran('tur-Latn')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/wordlistpronunciationdataset/turkish.txt'\n",
        "output_file = 'turkishpronunciations.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIhxe6wqkA3q",
        "outputId": "c8ca0994-d8f5-4961-aa51-52723cdce8b3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63840/63840 [00:02<00:00, 25573.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import epitran\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize the Uyghur-IPA transliteration tool\n",
        "epi = epitran.Epitran('uig-Arab')\n",
        "\n",
        "# Define the input and output file paths\n",
        "input_file = '/content/drive/MyDrive/wordlistpronunciationdataset/Sozler_all_in_one.txt'\n",
        "output_file = 'uyghurpronunciations.txt'\n",
        "\n",
        "# Read in the input file and iterate through the lines, transcribing each word and writing to the output file\n",
        "with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "    lines = infile.readlines()\n",
        "    for i, line in enumerate(tqdm(lines)):\n",
        "        # Remove any newline characters and split the line into individual words\n",
        "        line = line.strip()\n",
        "        words = line.split()\n",
        "        \n",
        "        # Transcribe each word and write the transcriptions to the output file\n",
        "        transcriptions = []\n",
        "        for word in words:\n",
        "            transcription = epi.transliterate(word)\n",
        "            transcriptions.append(transcription)\n",
        "        outfile.write('\\t'.join(transcriptions) + '\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maiB5fYckCAG",
        "outputId": "16a77e2b-04f8-4236-9829-293e8cc8f445"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 436684/436684 [00:28<00:00, 15205.32it/s]\n"
          ]
        }
      ]
    }
  ]
}