{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SRONPVNl_7_OA6BN7EAuBPt-lej9AKF8",
      "authorship_tag": "ABX9TyMQFNKcrlULWWs6CQ0TeWXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkuStudies/abraxalexicon/blob/main/updatejsons1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#UPDATE EXISTING JSON FILES\n",
        "import json\n",
        "\n",
        "# IPA mapping\n",
        "ipa_mapping = {\n",
        " #VOWELS 1.x\n",
        "# A sound\n",
        "    'e': 1.11,  # ei\n",
        "    'e̞': 1.111,  # ei\n",
        "    'ɛ': 1.1111,  # ei\n",
        "\n",
        "    'ɔ': 1.21,  # t_o_p #ah\n",
        "    'ɑ': 1.212,  # f_a_ther #ahh\n",
        "    'ɒ': 1.2131,  # t_o_p ahh\n",
        "    'a': 1.21311,  # f_a_ther uh\n",
        "    'ã': 1.214111,  # ah as in man\n",
        "    'ɤ̞': 1.2151111,  # oeh\n",
        "    'ɞ': 1.21622222,  # b_ir_d oeh\n",
        "    'ɶ': 1.217,  # ah\n",
        "    'ɐ': 1.218,  # aeh\n",
        "    'ə': 1.219,  # ah\n",
        "    'ɵ': 1.2185,  # b_ird\n",
        "    'á': 1.2174,\n",
        "    'æ': 1.2172,  # uh\n",
        "    'ä': 1.2163,  # uh\n",
        "    'ɘ': 1.2157,  # b_i_rd\n",
        "    'ʌ': 1.2146,  # b_u_tter uhh\n",
        "    'œ': 1.2138,  # g_o_ld\n",
        "  \n",
        "\n",
        "# eh sound\n",
        "    'ɜ': 1.31,  # h_e_rd\n",
        "    'ɤ': 1.311,  # s_urge\n",
        "   \n",
        "    \n",
        "\n",
        "# ooh sound  \n",
        "    'u': 1.61,  # f_oo_d\n",
        "    'ʏ': 1.611,  # hoo_k\n",
        "    'ũ': 1.612,\n",
        "    'ø': 1.613,  # ooh\n",
        "    'y': 1.614,  # f_ew\n",
        "    'ʉ': 1.615,  # u_nder ooh\n",
        "    'ɯ': 1.616,\n",
        "    'ʊ': 1.617,  # p_ut ooh\n",
        "    'ø̞': 1.618,  # h_u_rt ooh\n",
        "\n",
        "# CONSONANTS\n",
        "\n",
        "#b and p 2.x\n",
        "# b\n",
        "    'b̪': 2.111,  # ta_b_le (dental)\n",
        "    'ʙ̥': 2.112,  # e_b_b (voiceless)\n",
        "    'ʙ': 2.113,  # e_b_b\n",
        "    'b': 2.114,  # b_oy\n",
        "    'ɓ': 2.115,\n",
        "    'v': 2,516,  # v_oice\n",
        "    'ⱱ̟': 2.517,  # v_alveolar\n",
        "    'ⱱ': 2.518,  # v_alveolar\n",
        "    'β': 2.519,  # v_oice\n",
        "# fa/va\n",
        "    'ɸ': 2.81,  # f_ish\n",
        "    'f': 2.82,  # f_ish\n",
        "# g\n",
        "    'ɣ': 3.11,  # g_uy\n",
        "    'ɡ': 3.12,  # g_uy\n",
        "    'ɢ': 3.13,  # G_od (uvular)\n",
        "    'ɢ̆': 3.14,  # G_od\n",
        "    'g': 3.15,  # g_uy\n",
        "    'ɡ̃': 3.16,\n",
        "# d\n",
        "    'd̼': 4.11,  # s_u_d_den (linguolabial)\n",
        "    'd': 4.12,  # d_ime\n",
        "    'ɖ': 4.13,  # s_u_d_den\n",
        "    'd͡': 4.14,  # d_u_ally\n",
        "    'ɗ': 4.15,\n",
        "\n",
        "# h\n",
        "    'ħ': 5.11,  # Ha\n",
        "    'ʜ': 5.12,  # h_ow\n",
        "    'h': 5.13,  # h_at\n",
        "    'ɦ': 5.14,  # h_and\n",
        "    'ʢ': 5.15,  # fr_o_g  \n",
        "    'ʕ': 5.16,  # bra_c_e\n",
        "\n",
        "# w\n",
        "    'ɰ': 6.11,  # w_e\n",
        "    'w': 6.12,  # w_ood\n",
        "# y\n",
        "    'j': 6.51,  # y_e_s\n",
        "    'ɟ': 6.52,  # y_e_s (palatal) \n",
        "    'ʝ': 6.53,  # y_e_s\n",
        "    'ʄ': 6.54,\n",
        "\n",
        "# z\n",
        "    'z': 7.11,  # z_one\n",
        "    'ɮ': 7.12,  # z_one (voiced)\n",
        "    'ʒ': 7.13,  # mea_s_ure\n",
        "    'ʑ': 7.14,  # mea_s_ure (voiced)\n",
        "    'ʐ': 7.15,  # mea_s_ure (voiced)\n",
        "    'ʑ': 7.16,  # mea_s_ure (voiced)\n",
        "\n",
        "    'x': 8.11,  # lo_ch\n",
        "    'χ': 8.12,  # lo_ch (uvular)\n",
        "    'ç': 8.13,  # h_atch\n",
        "\n",
        "# th\n",
        "    'θ̼': 9.11,  # s_u_th (linguolabial)\n",
        "    'ð̼': 9.12,  # th_is (linguolabial)\n",
        "    'θ': 9.13,  # th_ink\n",
        "    'ð': 9.14,  # th_is\n",
        "    'θ̠': 9.15,  # th_in (retracted)\n",
        "    'ð̠': 9.16,  # th_en (retracted)\n",
        "\n",
        "# ee sound\n",
        "    'ɨ': 10.51,  # ee\n",
        "    'i': 10.511,  # f_ee_t\n",
        "    'ɪ': 10.512,  # ee\n",
        "    'ẽ': 10.513,\n",
        "# k\n",
        "    'k': 20.11,  # c_a_ke\n",
        "    'q': 20.12,  # c_o_t\n",
        "    'c': 20.13,  # k_e_k\n",
        "\n",
        "\n",
        "# l\n",
        "    'ʎ̝': 30.11,  # mi_ll_ion (palatalized lateral)\n",
        "    'l': 30.12,  # l_et\n",
        "    'ɭ': 30.13,  # mi_ll_ion\n",
        "    'ʎ': 30.14,  # mi_ll_ion (palatal)\n",
        "    'ʟ': 30.15,  # be_ll\n",
        "    'ʟ̠': 30.16,  # ve_ll_um (retracted)\n",
        "    'ʎ̆': 30.17,  # mi_ll_ion (extra-short)\n",
        "    'ʟ̆': 30.18,  # be_ll (extra-short)\n",
        "\n",
        "# m\n",
        "    'm̥': 40.11,  # s_m_ile (voiceless)\n",
        "    'm': 40.12,  # m_ine\n",
        "    'ɱ': 40.13,  # s_m_ile (nasal)\n",
        "\n",
        "# n\n",
        "    'ɳ̊': 50.11,  # t_ur_n (voiceless)\n",
        "    'n̼': 50.12,  # w_i_n (linguolabial)\n",
        "    'n̥': 50.13,  # s_n_ake (voiceless)\n",
        "    'n': 50.14,  # n_ine\n",
        "    'ɳ': 50.15,  # t_ur_n (nasal)\n",
        "    'ɲ̊': 50.16,  # si_n (voiceless)\n",
        "    'ɲ': 50.17,  # si_gn\n",
        "    'ŋ̊': 50.18,  # si_ng (voiceless)\n",
        "    'ŋ': 50.19,  # si_ng\n",
        "    'ɴ': 50.185,  # si_ng\n",
        "    'ñ': 50.175,\n",
        "\n",
        "    's': 60.11,  # s_un\n",
        "\n",
        "# oh sound\n",
        "    'o̞': 70.11,  # b_o_at\n",
        "    'o': 70.12,  # g_o   \n",
        "\n",
        "# p\n",
        "    'p': 80.11,  # p_en\n",
        "    'p̪': 80.12,  # s_p_in (dental)\n",
        "\n",
        "    'ɬ': 90.11,  # whi_st_le\n",
        "    'ꞎ': 90.12,  # whi_st_le\n",
        "\n",
        "    'ʡ̆': 100.14,  # gl_o_ttal\n",
        "    'ʡ': 100.15,  # gl_o_ttal\n",
        "\n",
        "# r\n",
        "    'ɹ̥': 200.11,  # b_r_ing (voiceless)\n",
        "    'ɹ̠̊˔': 200.12,  # th_r_ow (voiceless retroflex)\n",
        "    'ɹ̠˔': 200.13,  # th_r_ow (retroflex)\n",
        "    'ɻ̊˔': 200.14,  # th_r_ow (voiceless retroflex)\n",
        "    'ɻ˔': 200.15,  # th_r_ow (retroflex)\n",
        "    'r': 200.16,  # r_ed\n",
        "    'r̥': 200.17,  # r_ed (voiceless)\n",
        "    'ʀ̥': 200.18,  # r_ed (uvular voiceless)\n",
        "    'ʀ': 200.19,  # r_ed (uvular)\n",
        "    'ɺ̥': 200.20,  # flu_tt_er (voiceless)\n",
        "    'ɺ': 200.21,  # flu_tt_er\n",
        "    'ɹ': 200.22,  # r_ead\n",
        "    'ɻ': 200.23,  # r_ed\n",
        "    'ʁ': 200.24,  # r_ed\n",
        "    'ɽ̊': 200.25,  # bu_tt_er (voiceless retroflex)\n",
        "    'ɽ': 200.26,  # bu_tt_er (retroflex)\n",
        "    'ʟ̝': 200.27,  # lra\n",
        "    'ɾ̼': 200.28,  # bu_tt_er (linguolabial)\n",
        "    'ɾ̥': 200.29,  # bu_tt_er (voiceless)\n",
        "    'ɾ': 200.30,  # bu_tt_er\n",
        "\n",
        "# sh\n",
        "    'ʃ': 300.11,  # s_h_e\n",
        "    'ʂ': 300.12,  # s_h_ut    \n",
        "    'ɕ': 300.13,  # s_h_y\n",
        "\n",
        "\n",
        "# t\n",
        "    't': 400.11,  # t_ime\n",
        "    'ʈ': 400.12,  # s_t_rap\n",
        "    't̼': 400.13,  # s_u_t\n",
        "    't': 400.14,  # t_rap\n",
        "    't͡': 400.15\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "def process_json_files(input_files):\n",
        "    for input_file in input_files:\n",
        "        with open(input_file, 'r+') as file:\n",
        "            data = json.load(file)\n",
        "            rows = data['rows']\n",
        "            for row in rows:\n",
        "                pronunciation = row['pronunciation']\n",
        "\n",
        "                # Check for doubled characters and remove one of them\n",
        "                doubled_chars = set()\n",
        "                updated_pronunciation = \"\"\n",
        "                for char in pronunciation:\n",
        "                    if char in doubled_chars:\n",
        "                        continue\n",
        "                    if char * 2 in pronunciation:\n",
        "                        doubled_chars.add(char)\n",
        "                    updated_pronunciation += char\n",
        "\n",
        "                # Remove unwanted characters from pronunciation\n",
        "                updated_pronunciation = updated_pronunciation.replace(':', '').replace('ˌ', '').replace('̩', '').replace('̯', '').replace('|', '').replace('ˤ', '').replace('ː', '').replace('_', '').replace(' ', '').replace(']', '').replace('ʼ', '').replace('ʷ', '')\n",
        "\n",
        "                # Update embedding based on the updated pronunciation\n",
        "                embedding = []\n",
        "                for char in updated_pronunciation:\n",
        "                    if char in ipa_mapping:\n",
        "                        embedding.append(ipa_mapping[char])\n",
        "\n",
        "                # Pad the embedding if it is shorter than 18\n",
        "                if len(embedding) < 18:\n",
        "                    embedding.extend([0] * (18 - len(embedding)))\n",
        "\n",
        "                # Truncate the embedding if it is longer than 18\n",
        "                if len(embedding) > 18:\n",
        "                    embedding = embedding[:18]\n",
        "\n",
        "                # Update the row's fields\n",
        "                row['embedding'] = embedding\n",
        "                row['pronunciation'] = updated_pronunciation\n",
        "\n",
        "            # Move the file pointer to the beginning and truncate the file\n",
        "            file.seek(0)\n",
        "            file.truncate()\n",
        "\n",
        "            # Write the modified data back to the file\n",
        "            json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"JSON processing completed for '{input_file}'. Modified file: '{input_file}'\")\n",
        "\n",
        "input_files = ['/content/drive/MyDrive/Abraxa/mongolian.json', '/content/drive/MyDrive/Abraxa/scotsgaelic.json', '/content/drive/MyDrive/Abraxa/spanish.json', '/content/drive/MyDrive/Abraxa/swahili.json', '/content/drive/MyDrive/Abraxa/swedish.json', '/content/drive/MyDrive/Abraxa/tamil.json', '/content/drive/MyDrive/Abraxa/tk.json']\n",
        "process_json_files(input_files)\n"
      ],
      "metadata": {
        "id": "RGEf87VUyqkG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7baaa465-991b-43a5-ef4d-f570701267a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/mongolian.json'. Modified file: '/content/drive/MyDrive/Abraxa/mongolian.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/scotsgaelic.json'. Modified file: '/content/drive/MyDrive/Abraxa/scotsgaelic.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/spanish.json'. Modified file: '/content/drive/MyDrive/Abraxa/spanish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/swahili.json'. Modified file: '/content/drive/MyDrive/Abraxa/swahili.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/swedish.json'. Modified file: '/content/drive/MyDrive/Abraxa/swedish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/tamil.json'. Modified file: '/content/drive/MyDrive/Abraxa/tamil.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/Abraxa/tk.json'. Modified file: '/content/drive/MyDrive/Abraxa/tk.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# IPA mapping\n",
        "ipa_mapping = {\n",
        " #VOWELS 1.x\n",
        "# A sound\n",
        "    'e': 1.11,  # ei\n",
        "    'e̞': 1.111,  # ei\n",
        "    'ɛ': 1.1111,  # ei\n",
        "    'ɔ': 1.21,  # t_o_p #ah\n",
        "    'ɑ': 1.212,  # f_a_ther #ahh\n",
        "    'ɒ': 1.2131,  # t_o_p ahh\n",
        "    'a': 1.21311,  # f_a_ther uh\n",
        "    'ã': 1.214111,  # ah as in man\n",
        "    'ɤ̞': 1.2151111,  # oeh\n",
        "    'ɞ': 1.21622222,  # b_ir_d oeh\n",
        "    'ɶ': 1.217,  # ah\n",
        "    'ɐ': 1.218,  # aeh\n",
        "    'ə': 1.219,  # ah\n",
        "    'ɵ': 1.2185,  # b_ird\n",
        "    'á': 1.2174,\n",
        "    'æ': 1.2172,  # uh\n",
        "    'ä': 1.2163,  # uh\n",
        "    'ɘ': 1.2157,  # b_i_rd\n",
        "    'ʌ': 1.2146,  # b_u_tter uhh\n",
        "    'œ': 1.2138,  # g_o_ld\n",
        "# eh sound\n",
        "    'ɜ': 1.31,  # h_e_rd\n",
        "    'ɤ': 1.311,  # s_urge\n",
        "# ooh sound  \n",
        "    'u': 1.61,  # f_oo_d\n",
        "    'ʏ': 1.611,  # hoo_k\n",
        "    'ũ': 1.612,\n",
        "    'ø': 1.613,  # ooh\n",
        "    'y': 1.614,  # f_ew\n",
        "    'ʉ': 1.615,  # u_nder ooh\n",
        "    'ɯ': 1.616,\n",
        "    'ʊ': 1.617,  # p_ut ooh\n",
        "    'ø̞': 1.618,  # h_u_rt ooh\n",
        "# b\n",
        "    'b̪': 2.111,  # ta_b_le (dental)\n",
        "    'ʙ̥': 2.112,  # e_b_b (voiceless)\n",
        "    'ʙ': 2.113,  # e_b_b\n",
        "    'b': 2.114,  # b_oy\n",
        "    'ɓ': 2.115,\n",
        "    'v': 2.516,  # v_oice\n",
        "    'ⱱ̟': 2.517,  # v_alveolar\n",
        "    'ⱱ': 2.518,  # v_alveolar\n",
        "    'β': 2.519,  # v_oice\n",
        "# fa/va\n",
        "    'ɸ': 2.81,  # f_ish\n",
        "    'f': 2.82,  # f_ish\n",
        "# g\n",
        "    'ɣ': 3.11,  # g_uy\n",
        "    'ɡ': 3.12,  # g_uy\n",
        "    'ɢ': 3.13,  # G_od (uvular)\n",
        "    'ɢ̆': 3.14,  # G_od\n",
        "    'g': 3.15,  # g_uy\n",
        "    'ɡ̃': 3.16,\n",
        "# d\n",
        "    'd̼': 4.11,  # s_u_d_den (linguolabial)\n",
        "    'd': 4.12,  # d_ime\n",
        "    'ɖ': 4.13,  # s_u_d_den\n",
        "    'd͡': 4.14,  # d_u_ally\n",
        "    'ɗ': 4.15,\n",
        "# h\n",
        "    'ħ': 5.11,  # Ha\n",
        "    'ʜ': 5.12,  # h_ow\n",
        "    'h': 5.13,  # h_at\n",
        "    'ɦ': 5.14,  # h_and\n",
        "    'ʢ': 5.15,  # fr_o_g  \n",
        "    'ʕ': 5.16,  # bra_c_e\n",
        "# w\n",
        "    'ɰ': 6.11,  # w_e\n",
        "    'w': 6.12,  # w_ood\n",
        "# y\n",
        "    'j': 6.51,  # y_e_s\n",
        "    'ɟ': 6.52,  # y_e_s (palatal) \n",
        "    'ʝ': 6.53,  # y_e_s\n",
        "    'ʄ': 6.54,\n",
        "# z\n",
        "    'z': 7.11,  # z_one\n",
        "    'ɮ': 7.12,  # z_one (voiced)\n",
        "    'ʒ': 7.13,  # mea_s_ure\n",
        "    'ʑ': 7.14,  # mea_s_ure (voiced)\n",
        "    'ʐ': 7.15,  # mea_s_ure (voiced)\n",
        "    'ʑ': 7.16,  # mea_s_ure (voiced)\n",
        "    'x': 8.11,  # lo_ch\n",
        "    'χ': 8.12,  # lo_ch (uvular)\n",
        "    'ç': 8.13,  # h_atch\n",
        "# th\n",
        "    'θ̼': 9.11,  # s_u_th (linguolabial)\n",
        "    'ð̼': 9.12,  # th_is (linguolabial)\n",
        "    'θ': 9.13,  # th_ink\n",
        "    'ð': 9.14,  # th_is\n",
        "    'θ̠': 9.15,  # th_in (retracted)\n",
        "    'ð̠': 9.16,  # th_en (retracted)\n",
        "# ee sound\n",
        "    'ɨ': 10.51,  # ee\n",
        "    'i': 10.511,  # f_ee_t\n",
        "    'ɪ': 10.512,  # ee\n",
        "    'ẽ': 10.513,\n",
        "# k\n",
        "    'k': 20.11,  # c_a_ke\n",
        "    'q': 20.12,  # c_o_t\n",
        "    'c': 20.13,  # k_e_k\n",
        "# l\n",
        "    'ʎ̝': 30.11,  # mi_ll_ion (palatalized lateral)\n",
        "    'l': 30.12,  # l_et\n",
        "    'ɭ': 30.13,  # mi_ll_ion\n",
        "    'ʎ': 30.14,  # mi_ll_ion (palatal)\n",
        "    'ʟ': 30.15,  # be_ll\n",
        "    'ʟ̠': 30.16,  # ve_ll_um (retracted)\n",
        "    'ʎ̆': 30.17,  # mi_ll_ion (extra-short)\n",
        "    'ʟ̆': 30.18,  # be_ll (extra-short)\n",
        "# m\n",
        "    'm̥': 40.11,  # s_m_ile (voiceless)\n",
        "    'm': 40.12,  # m_ine\n",
        "    'ɱ': 40.13,  # s_m_ile (nasal)\n",
        "# n\n",
        "    'ɳ̊': 50.11,  # t_ur_n (voiceless)\n",
        "    'n̼': 50.12,  # w_i_n (linguolabial)\n",
        "    'n̥': 50.13,  # s_n_ake (voiceless)\n",
        "    'n': 50.14,  # n_ine\n",
        "    'ɳ': 50.15,  # t_ur_n (nasal)\n",
        "    'ɲ̊': 50.16,  # si_n (voiceless)\n",
        "    'ɲ': 50.17,  # si_gn\n",
        "    'ŋ̊': 50.18,  # si_ng (voiceless)\n",
        "    'ŋ': 50.19,  # si_ng\n",
        "    'ɴ': 50.185,  # si_ng\n",
        "    'ñ': 50.175,\n",
        "    's': 60.11,  # s_un\n",
        "# oh sound\n",
        "    'o̞': 70.11,  # b_o_at\n",
        "    'o': 70.12,  # g_o   \n",
        "# p\n",
        "    'p': 80.11,  # p_en\n",
        "    'p̪': 80.12,  # s_p_in (dental)\n",
        "    'ɬ': 90.11,  # whi_st_le\n",
        "    'ꞎ': 90.12,  # whi_st_le\n",
        "    'ʡ̆': 100.14,  # gl_o_ttal\n",
        "    'ʡ': 100.15,  # gl_o_ttal\n",
        "# r\n",
        "    'ɹ̥': 200.11,  # b_r_ing (voiceless)\n",
        "    'ɹ̠̊˔': 200.12,  # th_r_ow (voiceless retroflex)\n",
        "    'ɹ̠˔': 200.13,  # th_r_ow (retroflex)\n",
        "    'ɻ̊˔': 200.14,  # th_r_ow (voiceless retroflex)\n",
        "    'ɻ˔': 200.15,  # th_r_ow (retroflex)\n",
        "    'r': 200.16,  # r_ed\n",
        "    'r̥': 200.17,  # r_ed (voiceless)\n",
        "    'ʀ̥': 200.18,  # r_ed (uvular voiceless)\n",
        "    'ʀ': 200.19,  # r_ed (uvular)\n",
        "    'ɺ̥': 200.20,  # flu_tt_er (voiceless)\n",
        "    'ɺ': 200.21,  # flu_tt_er\n",
        "    'ɹ': 200.22,  # r_ead\n",
        "    'ɻ': 200.23,  # r_ed\n",
        "    'ʁ': 200.24,  # r_ed\n",
        "    'ɽ̊': 200.25,  # bu_tt_er (voiceless retroflex)\n",
        "    'ɽ': 200.26,  # bu_tt_er (retroflex)\n",
        "    'ʟ̝': 200.27,  # lra\n",
        "    'ɾ̼': 200.28,  # bu_tt_er (linguolabial)\n",
        "    'ɾ̥': 200.29,  # bu_tt_er (voiceless)\n",
        "    'ɾ': 200.30,  # bu_tt_er\n",
        "# sh\n",
        "    'ʃ': 300.11,  # s_h_e\n",
        "    'ʂ': 300.12,  # s_h_ut    \n",
        "    'ɕ': 300.13,  # s_h_y\n",
        "# t\n",
        "    't': 400.11,  # t_ime\n",
        "    'ʈ': 400.12,  # s_t_rap\n",
        "    't̼': 400.13,  # s_u_t\n",
        "    't': 400.14,  # t_rap\n",
        "    't͡': 400.15\n",
        "}\n",
        "\n",
        "def process_json_files(input_folder, output_folder):\n",
        "    # Create the output folder if it doesn't exist\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Iterate over all files in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        input_file = os.path.join(input_folder, filename)\n",
        "        output_file = os.path.join(output_folder, filename)\n",
        "\n",
        "        # Skip directories and non-JSON files\n",
        "        if os.path.isdir(input_file) or not filename.endswith('.json'):\n",
        "            continue\n",
        "\n",
        "        with open(input_file, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            rows = data['rows']\n",
        "\n",
        "            for row in rows:\n",
        "                pronunciation = row['pronunciation']\n",
        "\n",
        "                # Check for doubled characters and remove one of them\n",
        "                doubled_chars = set()\n",
        "                updated_pronunciation = \"\"\n",
        "                for char in pronunciation:\n",
        "                    if char in doubled_chars:\n",
        "                        continue\n",
        "                    if char * 2 in pronunciation:\n",
        "                        doubled_chars.add(char)\n",
        "                    updated_pronunciation += char\n",
        "\n",
        "                # Remove unwanted characters from pronunciation\n",
        "                updated_pronunciation = updated_pronunciation.replace(':', '').replace('ˌ', '').replace('̩', '').replace('̯', '').replace('|', '').replace('ˤ', '').replace('ː', '').replace('_', '').replace(' ', '').replace(']', '').replace('ʼ', '').replace('ʷ', '')\n",
        "\n",
        "                # Update embedding based on the updated pronunciation\n",
        "                embedding = []\n",
        "                for char in updated_pronunciation:\n",
        "                    if char in ipa_mapping:\n",
        "                        embedding.append(ipa_mapping[char])\n",
        "\n",
        "                # Pad the embedding if it is shorter than 18\n",
        "                if len(embedding) < 18:\n",
        "                    embedding.extend([0] * (18 - len(embedding)))\n",
        "\n",
        "                # Truncate the embedding if it is longer than 18\n",
        "                if len(embedding) > 18:\n",
        "                    embedding = embedding[:18]\n",
        "\n",
        "                # Update the row's fields\n",
        "                row['embedding'] = embedding\n",
        "                row['pronunciation'] = updated_pronunciation\n",
        "\n",
        "            with open(output_file, 'w') as outfile:\n",
        "                json.dump(data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "        print(f\"JSON processing completed for '{input_file}'. Modified file: '{output_file}'\")\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/AbraxaLEXICONcompletejson'\n",
        "output_folder = '/content/gematriaembedded'\n",
        "\n",
        "process_json_files(input_folder, output_folder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMFrbqyWwsTN",
        "outputId": "3456f3dc-4e9d-4688-a520-f04b62be6cff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/amharic.json'. Modified file: '/content/gematriaembedded/amharic.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic.json'. Modified file: '/content/gematriaembedded/arabic.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic2.json'. Modified file: '/content/gematriaembedded/arabic2.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic3.json'. Modified file: '/content/gematriaembedded/arabic3.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic4.json'. Modified file: '/content/gematriaembedded/arabic4.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic5.json'. Modified file: '/content/gematriaembedded/arabic5.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/arabic6.json'. Modified file: '/content/gematriaembedded/arabic6.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/araguari.json'. Modified file: '/content/gematriaembedded/araguari.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/basque.json'. Modified file: '/content/gematriaembedded/basque.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/bu.json'. Modified file: '/content/gematriaembedded/bu.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/chinese.json'. Modified file: '/content/gematriaembedded/chinese.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/danish.json'. Modified file: '/content/gematriaembedded/danish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/du.json'. Modified file: '/content/gematriaembedded/du.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/en.json'. Modified file: '/content/gematriaembedded/en.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/fa.json'. Modified file: '/content/gematriaembedded/fa.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/fr.json'. Modified file: '/content/gematriaembedded/fr.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/ge.json'. Modified file: '/content/gematriaembedded/ge.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/ge2.json'. Modified file: '/content/gematriaembedded/ge2.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/ge3.json'. Modified file: '/content/gematriaembedded/ge3.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/greek.json'. Modified file: '/content/gematriaembedded/greek.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/hebrew.json'. Modified file: '/content/gematriaembedded/hebrew.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/hi.json'. Modified file: '/content/gematriaembedded/hi.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/irish.json'. Modified file: '/content/gematriaembedded/irish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/japanese.json'. Modified file: '/content/gematriaembedded/japanese.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/latin.json'. Modified file: '/content/gematriaembedded/latin.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/maori.json'. Modified file: '/content/gematriaembedded/maori.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/spanish.json'. Modified file: '/content/gematriaembedded/spanish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/swahili.json'. Modified file: '/content/gematriaembedded/swahili.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/swedish.json'. Modified file: '/content/gematriaembedded/swedish.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/tamil.json'. Modified file: '/content/gematriaembedded/tamil.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/tk.json'. Modified file: '/content/gematriaembedded/tk.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/korean.json'. Modified file: '/content/gematriaembedded/korean.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/icelandic.json'. Modified file: '/content/gematriaembedded/icelandic.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/mongolian.json'. Modified file: '/content/gematriaembedded/mongolian.json'\n",
            "JSON processing completed for '/content/drive/MyDrive/AbraxaLEXICONcompletejson/scotsgaelic.json'. Modified file: '/content/gematriaembedded/scotsgaelic.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def find_long_fields(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    \n",
        "    long_fields = []\n",
        "    for row in data['rows']:\n",
        "        for key, value in row.items():\n",
        "            if isinstance(value, str) and len(value) > 99:\n",
        "                long_fields.append({'id': row['id'], 'field': key, 'length': len(value)})\n",
        "    \n",
        "    return long_fields\n",
        "\n",
        "# Usage example\n",
        "json_file = '/content/drive/MyDrive/Abraxa/korean.json'  # Replace with your JSON file path\n",
        "\n",
        "long_fields = find_long_fields(json_file)\n",
        "for field in long_fields:\n",
        "    print(f\"ID: {field['id']}, Field: {field['field']}, Length: {field['length']}\")\n"
      ],
      "metadata": {
        "id": "btDESoAyEAC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3307fd9f-77ca-41c1-dfb2-1000988ba653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ID: korean_14235, Field: pronunciation, Length: 182\n"
          ]
        }
      ]
    }
  ]
}