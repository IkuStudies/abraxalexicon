{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fgvhHK5IbTceaORtWGUvc6MB_kutVfHU",
      "authorship_tag": "ABX9TyPq8foXCbYtB5iphRQTqVwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IkuStudies/abraxalexicon/blob/main/IPA2VEC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "iplBf5Fh3BSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IJR6Ngja288L"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# IPA mapping dictionary\n",
        "ipa_mapping = {\n",
        "    'ʉ': 1, 'ɯ': 1, 'u': 1, 'ʏ': 1, 'ʊ': 1,\n",
        "    'ɵ': 2, 'ɤ': 2, 'o': 2, 'ə': 2, 'ɤ̞': 2,\n",
        "    'e̞': 3, 'ø̞': 3, 'ɛ': 3, 'œ': 3, 'ɶ': 3, 'ɜ': 3, 'ɞ': 3, 'e': 3, 'ø': 3, 'ɘ': 3, 'ɪ': 3, 'i': 3, 'y': 3, 'ɨ': 3,\n",
        "    'ɐ': 4, 'ʌ': 4, 'ʡ': 4, 'ʔ': 4, 'ʔ̞': 4, 'ʡ̆': 4,\n",
        "    'æ': 5, 'a': 5, 'ä': 5,\n",
        "    'o̞': 6, 'ɔ': 6, 'ɑ': 6, 'ɒ': 6,\n",
        "    'm̥': 7, 'm': 7, 'ɱ': 7, 'ɳ̊': 7,\n",
        "    'n̼': 8, 'n̥': 8, 'n': 8, 'ɳ': 8, 'ɲ̊': 8, 'ɲ': 8, 'ŋ̊': 8, 'ŋ': 8, 'ɴ': 8, 'n': 8,\n",
        "    'p': 9, 'b': 9, 'p̪': 9, 'b̪': 9, 'ʙ̥': 9, 'ʙ': 9, 'ɹ̥': 9,\n",
        "    'd̼': 10, 'd': 10, 'ɖ': 10,\n",
        "    't': 11, 'ʈ': 11, 't̼': 11,\n",
        "    'j': 12, 'ɟ': 12,\n",
        "    'k': 13, 'q': 13, 'x': 13, 'χ': 13, 'ħ': 13, 'c': 13,\n",
        "    'ɣ': 14, 'ɡ': 14, 'ɢ': 14, 'ɢ̆': 14,\n",
        "    'z': 15, 'ɮ': 15, 'ʃ': 15, 'ʒ': 15, 'ʂ': 15, 'ʐ': 15, 'ɕ': 15, 'ʑ': 15, 'ʝ': 15, 'ɕ': 15, 'ʑ': 15, 's': 15,\n",
        "    'ɸ': 16, 'β': 16, 'f': 16, 'v': 16, 'ⱱ̟': 16, 'ⱱ': 16,\n",
        "    'θ̼': 17, 'ð̼': 17, 'θ': 17, 'ð': 17, 'θ̠': 17, 'ð̠': 17, 'ɹ̠̊˔': 17, 'ɹ̠˔': 17, 'ɻ̊˔': 17, 'ɻ˔': 17, 'th': 17,\n",
        "    'ɺ̥': 18, 'ɺ': 18, 'ɾ̼': 18, 'ɾ̥': 18, 'ɾ': 18, 'ɽ̊': 18, 'ɽ': 18, 'ɹ': 18, 'ɻ': 18, 'ʁ': 18,\n",
        "    'ç': 19, 'ʕ': 19, 'h': 19, 'ɦ': 19,\n",
        "    'ɰ': 20, 'w': 20,\n",
        "    'r': 21, 'r̥': 21, 'ɽ̊r̥': 21, 'ɽr': 21, 'ʀ̥': 21, 'ʀ': 21,\n",
        "    'ʜ': 22, 'ʢ': 22,\n",
        "    'ɬ': 23, 'ꞎ': 23, 'ʎ̝': 23, 'ʟ̝': 23, 'l': 23, 'ɭ': 23, 'ʎ': 23, 'ʟ': 23, 'ʟ̠': 23, 'ʎ̆': 23, 'ʟ̆': 23\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Custom JSON encoder to handle NumPy arrays\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "# Function to transcribe IPA string to numerical values using ipa_mapping\n",
        "def transcribe_ipa(ipa_string):\n",
        "    ipa_transcription = []\n",
        "    for char in ipa_string:\n",
        "        if char in ipa_mapping:\n",
        "            ipa_transcription.append(ipa_mapping[char])\n",
        "    return np.array(ipa_transcription)\n",
        "\n",
        "input_file = '/content/drive/MyDrive/boyah/turkish.json'\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/updated_turkish.json'\n",
        "max_length = 18  # Maximum length for numerical representations\n",
        "\n",
        "# Open input and output files\n",
        "with open(input_file, 'r') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "    data = json.load(file_in)\n",
        "    rows = data['rows']\n",
        "    for row in rows:\n",
        "        word = row['word']\n",
        "        pronunciation = row['pronunciation']\n",
        "        embedding = row['embedding']\n",
        "        \n",
        "        # Convert pronunciation to numerical representation\n",
        "        transcription = transcribe_ipa(pronunciation)\n",
        "\n",
        "        # Truncate the transcription if the length exceeds the maximum\n",
        "        if len(transcription) > max_length:\n",
        "            transcription = transcription[:max_length]\n",
        "\n",
        "        # Pad the transcription values with zeros to match the maximum length\n",
        "        padded_transcription = np.pad(transcription, (0, max_length - len(transcription)))\n",
        "\n",
        "        # Replace the incorrect embedding with the correct embedding\n",
        "        row['embedding'] = padded_transcription.tolist()\n",
        "\n",
        "    # Write the updated data to the output file with formatted indentation and non-ASCII characters preserved\n",
        "    json.dump(data, file_out, cls=NumpyEncoder, indent=4, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "input_file = '/content/drive/MyDrive/Colab Notebooks/updated_turkish.json'\n",
        "output_file = '/content/drive/MyDrive/Colab Notebooks/tk.json'\n",
        "code = 'tk_'  # Code to add in front of the number in \"id\"\n",
        "\n",
        "# Open input and output files\n",
        "with open(input_file, 'r', encoding='utf-8') as file_in, open(output_file, 'w', encoding='utf-8') as file_out:\n",
        "    data = json.load(file_in)\n",
        "    rows = data['rows']\n",
        "    for row in rows:\n",
        "        entity_id = row['id']\n",
        "        modified_id = f'{code}{entity_id}'\n",
        "        row['id'] = modified_id\n",
        "\n",
        "    # Write the updated data to the output file with formatted indentation and non-ASCII characters preserved\n",
        "    json.dump(data, file_out, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "id": "kEnt0-NeaF5g"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "\n",
        "def split_json(input_file, output_prefix, num_files):\n",
        "    with open(input_file, 'r') as input_file:\n",
        "        data = json.load(input_file)\n",
        "        total_rows = len(data[\"rows\"])\n",
        "        rows_per_file = math.ceil(total_rows / num_files)\n",
        "\n",
        "        for i in range(num_files):\n",
        "            start_index = i * rows_per_file\n",
        "            end_index = min((i + 1) * rows_per_file, total_rows)\n",
        "            subset_data = {\n",
        "                \"rows\": data[\"rows\"][start_index:end_index]\n",
        "            }\n",
        "            output_file = f\"{output_prefix}_{i}.json\"\n",
        "            with open(output_file, 'w') as output_file:\n",
        "                json.dump(subset_data, output_file, indent=4)\n",
        "\n",
        "# Example usage\n",
        "input_file_path = '/content/drive/MyDrive/boyah/german_milvus_formatted.json'\n",
        "output_prefix = 'german'\n",
        "num_files = 3\n",
        "\n",
        "# Split the JSON file into multiple files\n",
        "split_json(input_file_path, output_prefix, num_files)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sZ0MER1Qd6QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def fix_json_field_names(json_data):\n",
        "    fixed_data = []\n",
        "    for row in json_data[\"rows\"]:\n",
        "        fixed_row = {}\n",
        "        for key, value in row.items():\n",
        "            fixed_key = key.strip()  # Remove leading/trailing spaces from the field name\n",
        "            fixed_row[fixed_key] = value\n",
        "        fixed_data.append(fixed_row)\n",
        "    json_data[\"rows\"] = fixed_data\n",
        "    return json_data\n",
        "\n",
        "# Read the JSON file with UTF-8 encoding\n",
        "with open(\"/content/drive/MyDrive/boyah/arab2_2.json\", encoding=\"utf-8\") as file:\n",
        "    json_content = json.load(file)\n",
        "\n",
        "# Fix the field names\n",
        "fixed_json = fix_json_field_names(json_content)\n",
        "\n",
        "# Write the fixed JSON to a new file with UTF-8 encoding\n",
        "with open(\"arab6.json\", \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(fixed_json, file, indent=4, ensure_ascii=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "uBSqjZr-iDAj",
        "outputId": "a9b90c80-6245-4fbe-bb9b-8b72e652a2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-819552cfe6df>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Write the fixed JSON to a new file with UTF-8 encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arab6.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfixed_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/zilliztech/cloud-vectordb-examples.git\n",
        "pip3 install pymilvus\n",
        "cd cloud-vectordb-examples\n",
        "cd python\n",
        "\n",
        "change this config.ini file uri = https://in01-daf1771fc27e45a.aws-us-west-2.vectordb.zillizcloud.com:19542\n",
        "user = db_admin\n",
        "password = ******\n",
        "secure = True\n",
        "python3 hello_zilliz_vectordb.py\n",
        "\n",
        "should be similarlooking to this \n",
        "Connecting to DB: https://in01-daf1771fc27e45a.aws-us-west-2.vectordb.zillizcloud.com:19542\n",
        "Success!\n",
        "Creating example collection: book\n",
        "Schema: {...}\n",
        "Success!\n",
        "Inserting 100000 entities... \n",
        "Succeed in 6.0288 seconds!\n",
        "Building AutoIndex...\n",
        "Succeed in 18.9118 seconds!\n",
        "Loading collection...\n",
        "Succeed in 2.5229 seconds!\n",
        "Searching vector:[[...][...]...]\n",
        "search 0 latency: 0.0057 seconds!\n",
        "Searching vector:[[...][...]...]\n",
        "search 1 latency: 0.0049 seconds!\n",
        "Searching vector:[[...][...]...]\n",
        "search 2 latency: 0.0051 seconds!\n",
        "...\n",
        "..."
      ],
      "metadata": {
        "id": "BXZPIqP89N4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}